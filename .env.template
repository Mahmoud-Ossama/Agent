# Environment Configuration Template
# Copy this file to .env and fill in your actual values

# ===========================================
# LLM Provider Configuration
# ===========================================

# Choose your LLM provider: openai, gemini, or ollama
LLM_PROVIDER=gemini

# ===========================================
# API Keys (fill in based on your provider)
# ===========================================

# For Google Gemini
GEMINI_API_KEY=your_gemini_api_key_here

# For OpenAI
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ENGINE=gpt-4
OPENAI_MAX_TOKENS=2000
OPENAI_TEMPERATURE=0.7

# For local Ollama
OLLAMA_URL=http://localhost:11434

# ===========================================
# Agent Configuration (Optional)
# ===========================================

# Maximum iterations per testing stage
MAX_ITERATIONS_PER_STAGE=5

# Command execution timeout (seconds)
COMMAND_TIMEOUT=300

# Enable debug mode (true/false)
DEBUG_MODE=false

# Default output directory
OUTPUT_DIR=results

# ===========================================
# Security Settings
# ===========================================

# Enable command validation (recommended: true)
ENABLE_COMMAND_VALIDATION=true

# Enable safe mode (blocks potentially dangerous commands)
SAFE_MODE=true

# ===========================================
# Logging Configuration
# ===========================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable file logging
LOG_TO_FILE=true

# Log file format
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
