# Enhanced AI-Powered Penetration Testing Agent

An intelligent penetration testing agent that uses Large Language Models (LLM) to dynamically decide which commands to execute during security testing. Instead of static, pre-defined tools, this agent asks the LLM what commands to run based on current findings and context.

## ðŸŒŸ Key Features

### Dynamic Command Execution
- **AI-Driven Decisions**: LLM analyzes findings and decides next actions
- **Context-Aware**: Commands chosen based on current stage and discoveries
- **Adaptive Strategy**: Changes approach based on results
- **No Static Tools**: All commands are dynamically generated

### Intelligent Analysis
- **Real-time Vulnerability Analysis**: AI analyzes outputs for vulnerabilities
- **SQL Injection Focus**: Specialized in detecting and exploiting SQL injection
- **Risk Assessment**: Automatic severity classification
- **Smart Reporting**: AI-generated comprehensive reports

### Three Operating Modes

#### 1. Intelligent Mode (Recommended)
- LLM makes strategic decisions about testing approach
- Analyzes findings between commands
- Provides insights and recommendations
- Generates professional reports

#### 2. Dynamic Mode
- Focuses on command execution
- LLM suggests specific terminal commands
- Executes commands safely with validation
- Maintains detailed command history

#### 3. MCP Parallel Mode (New!)
- **Multi-Terminal Execution**: Runs commands across multiple terminals simultaneously
- **Intelligent Parallelization**: Automatically determines which commands can run in parallel
- **Professional Methodology**: Follows industry-standard pentesting phases
- **Maximum Efficiency**: Reduces testing time by 60-80% through parallel operations
- **Human-like Simulation**: Maintains realistic typing patterns across all terminals

## ðŸš€ Quick Start

### Prerequisites
```bash
pip install -r requirements.txt
```

### Environment Setup
Create a `.env` file with your LLM provider configuration:

```env
# Choose your LLM provider
LLM_PROVIDER=gemini  # or openai, ollama

# API Keys (depending on provider)
GEMINI_API_KEY=your_gemini_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Ollama local server
OLLAMA_URL=http://localhost:11434
```

### Basic Usage

#### Intelligent Mode (Default)
```bash
python execution/run_enhanced_agent.py --target http://example.com
```

#### Dynamic Mode
```bash
python execution/run_enhanced_agent.py --target example.com --mode dynamic
```

#### MCP Parallel Mode (Multi-Terminal)
```bash
python execution/run_enhanced_agent.py --target example.com --mode mcp_parallel --max-terminals 4
```

```
pentest-agent-enhanced/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ main_agent.py
â”‚   â”œâ”€â”€ dynamic_agent.py
â”‚   â”œâ”€â”€ mcp_agent.py
â”‚   â”œâ”€â”€ enhanced_mcp_agent.py          # NEW: Multi-terminal parallel agent
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ sqli_prompt.txt
â”‚   â”‚   â”œâ”€â”€ dynamic_prompt.txt
â”‚   â”‚   â””â”€â”€ parallel_prompt.txt        # NEW: Parallel execution prompts
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â”œâ”€â”€ attack_chain.py
â”‚   â”‚   â””â”€â”€ intelligent_chain.py
â”‚   â””â”€â”€ memory/
â”‚       â””â”€â”€ agent_memory.json
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ llm_interface.py
â”œâ”€â”€ execution/
â”‚   â””â”€â”€ run_enhanced_agent.py
â”œâ”€â”€ mcp_server.py
â”œâ”€â”€ results/
â”œâ”€â”€ run_agent.bat
â”œâ”€â”€ run_agent.sh
â”œâ”€â”€ run_parallel_agent.bat             # NEW: Windows parallel execution
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.template
â”œâ”€â”€ mcp.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ MCP_README.md
â”œâ”€â”€ EXAMPLES.md
â””â”€â”€ PARALLEL_EXAMPLES.md               # NEW: Parallel execution examples
```

## Getting Started

1. Install dependencies:

```bash
pip install -r requirements.txt
```

2. Configure your LLM API keys and settings in `configs/.env`.

3. Build and run the Docker container (optional):

```bash
docker build -t pentest-agent-sqli .
docker run --rm -it pentest-agent-sqli
```

4. Run the agent against a target:

```bash
python execution/run_agent.py --target https://testphp.vulnweb.com
```

5. View generated reports in the `results/` folder.

## Notes

- The agent uses dynamic reasoning chains and memory to simulate human-like penetration testing.
- It supports multiple LLM backends and can be extended with new tools easily.
- The agent is designed for high reliability and realism with no static branching logic.

## License

MIT License

## ðŸš€ New Parallel Execution Features

### Multi-Terminal Parallel Operations
The enhanced MCP mode now supports parallel command execution across multiple terminals, following professional penetration testing methodology:

#### Parallel Execution Strategy:

**Phase 1: Reconnaissance** âš¡ Full Parallel
- All commands execute simultaneously
- No dependencies between commands
- Maximum parallelization efficiency

**Phase 2: Enumeration** âš¡ Parallel after Recon
- Commands run in parallel once reconnaissance data is available
- Leverages discovered domains and IPs

**Phase 3: Vulnerability Analysis** âš¡ Parallel after Enumeration
- Runs after service enumeration completes
- Targeted vulnerability scanning based on discovered services

**Phase 4: Exploitation** âš¡ Targeted Parallel
- Parallel exploitation based on confirmed vulnerabilities
- Intelligent targeting of discovered attack vectors

#### Performance Benefits:
- **60-80% faster** execution time
- **4x parallel efficiency** with default 4-terminal setup
- **Intelligent resource management** prevents conflicts
- **Professional methodology** follows industry standards

#### Usage Examples:

```bash
# Basic parallel mode (4 terminals)
python execution/run_enhanced_agent.py --target example.com --mode mcp_parallel

# High-performance mode (8 terminals)
python execution/run_enhanced_agent.py --target example.com --mode mcp_parallel --max-terminals 8

# Conservative mode (2 terminals)
python execution/run_enhanced_agent.py --target example.com --mode mcp_parallel --max-terminals 2
```
